{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e01e06-4bdd-45d7-8d0c-45f88b7c80e3",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1f8082-ceda-42a9-a133-b26a82202ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "-- Attaching packages --------------------------------------- tidyverse 1.2.1 --\n",
      "v ggplot2 3.1.1       v purrr   0.3.2  \n",
      "v tibble  2.1.1       v dplyr   0.8.0.1\n",
      "v tidyr   0.8.3       v stringr 1.4.0  \n",
      "v readr   1.3.1       v forcats 0.4.0  \n",
      "-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x dplyr::filter() masks stats::filter()\n",
      "x dplyr::lag()    masks stats::lag()\n",
      "Registered S3 method overwritten by 'xts':\n",
      "  method     from\n",
      "  as.zoo.xts zoo \n",
      "Registered S3 method overwritten by 'quantmod':\n",
      "  method            from\n",
      "  as.zoo.data.frame zoo \n",
      "Registered S3 methods overwritten by 'forecast':\n",
      "  method             from    \n",
      "  fitted.fracdiff    fracdiff\n",
      "  residuals.fracdiff fracdiff\n",
      "\n",
      "Attaching package: 'data.table'\n",
      "\n",
      "The following objects are masked from 'package:dplyr':\n",
      "\n",
      "    between, first, last\n",
      "\n",
      "The following object is masked from 'package:purrr':\n",
      "\n",
      "    transpose\n",
      "\n",
      "\n",
      "Attaching package: 'foreach'\n",
      "\n",
      "The following objects are masked from 'package:purrr':\n",
      "\n",
      "    accumulate, when\n",
      "\n",
      "Loading required package: iterators\n",
      "Loading required package: parallel\n",
      "Loading required package: lattice\n",
      "\n",
      "Attaching package: 'caret'\n",
      "\n",
      "The following object is masked from 'package:purrr':\n",
      "\n",
      "    lift\n",
      "\n",
      "randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    combine\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n",
      "Warning message:\n",
      "\"package 'xgboost' was built under R version 3.6.3\"\n",
      "Attaching package: 'xgboost'\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    slice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "library(forecast)\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(data.table)\n",
    "library(IRdisplay)\n",
    "\n",
    "library(foreach)\n",
    "library(doParallel)\n",
    "\n",
    "library(caret)\n",
    "library(randomForest)\n",
    "library(xgboost)\n",
    "library(progress)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b7ce4-6423-4b3c-90dd-a9903770cdd0",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d6cfa-91a0-4093-8bb0-81c707cc5f38",
   "metadata": {},
   "source": [
    "### Display tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea54d8e-eae5-49a5-bfe3-583a83d49013",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Custom display function for the first and last 5 rows or full table if <= 20 rows\n",
    "display_limited <- function(dt) {\n",
    "  n <- nrow(dt)\n",
    "  \n",
    "  # If there are 20 or fewer rows, display the full table\n",
    "  if (n <= 20) {\n",
    "    limited_dt <- dt\n",
    "  } else {\n",
    "    # Otherwise, concatenate the first 5 rows, '...' and the last 5 rows\n",
    "    limited_dt <- rbind(head(dt, 5), as.list(rep(\"...\", ncol(dt))), tail(dt, 5))\n",
    "  }\n",
    "  \n",
    "  # Generate raw HTML manually\n",
    "  html_output <- paste0(\n",
    "    \"<table border='1' style='border-collapse:collapse;'>\",\n",
    "    \"<thead><tr>\",\n",
    "    paste0(\"<th>\", colnames(limited_dt), \"</th>\", collapse = \"\"),\n",
    "    \"</tr></thead>\",\n",
    "    \"<tbody>\",\n",
    "    paste0(\n",
    "      apply(limited_dt, 1, function(row) {\n",
    "        paste0(\"<tr>\", paste0(\"<td>\", row, \"</td>\", collapse = \"\"), \"</tr>\")\n",
    "      }),\n",
    "      collapse = \"\"\n",
    "    ),\n",
    "    \"</tbody></table>\"\n",
    "  )\n",
    "  \n",
    "  # Display the HTML in the Jupyter notebook\n",
    "  display_html(html_output)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca07cd-1494-4718-accf-a2b02a833a62",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddcd710d-cf6d-44d1-a834-d4b2dcbd321e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "calculate_metrics <- function(R_t, R_hat_t, individual) {\n",
    "  # Ensure the inputs are numeric vectors and individual is a dataframe\n",
    "  if (!is.numeric(R_t) || !is.numeric(R_hat_t)) {\n",
    "    stop(\"Both R_t and R_hat_t need to be numeric vectors.\")\n",
    "  }\n",
    "  \n",
    "  # Calculate metrics\n",
    "  mae <- mean(abs(R_t - R_hat_t), na.rm = TRUE)\n",
    "  rmse <- sqrt(mean((R_t - R_hat_t)^2, na.rm = TRUE))\n",
    "  mape <- mean(abs((R_t - R_hat_t) / R_t), na.rm = TRUE) * 100\n",
    "  r_squared <- ifelse(all(R_t == R_hat_t), 1, summary(lm(R_t ~ R_hat_t))$r.squared)\n",
    "  \n",
    "  # Create a data frame to hold the metrics and values\n",
    "  metrics_table <- data.frame(\n",
    "    MAE = mae,\n",
    "    RMSE = rmse,\n",
    "    MAPE = mape,\n",
    "    R_squared = r_squared\n",
    "  )\n",
    "  \n",
    "  # Return the metrics table\n",
    "  return(metrics_table)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e013225-897a-4e2f-a074-2b148d02d542",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1341d2c9-5d81-4d27-b872-71b47b97e9be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prepare_X_t <- function(individual) {\n",
    "  # Ensure the input is a dataframe\n",
    "  if (!is.data.frame(individual)) {\n",
    "    stop(\"The input must be a dataframe.\")\n",
    "  }\n",
    "  \n",
    "  # Extract hour from start_time and create a 'time_of_day' column\n",
    "  individual$time_of_day <- format(as.POSIXct(individual$HourDK), \"%H:%M:%S\")\n",
    "  \n",
    "  # Exclude specified columns but keep 'time_of_day'\n",
    "  X_t <- subset(individual, select = -c(HourDK, GrossConsumptionMWh))\n",
    "  \n",
    "  # Convert month, weekday, and time_of_day to factors with a reference category\n",
    "  X_t$month <- relevel(as.factor(X_t$MonthOfYear), ref = \"December\")  # Set December as reference\n",
    "  X_t$weekday <- relevel(as.factor(X_t$DayOfWeek), ref = \"Sunday\")   # Set Sunday as reference \n",
    "  X_t$time_of_day <- relevel(as.factor(X_t$Hour), ref = \"0\")         # Set 23 (11 PM) as reference\n",
    "\n",
    "  # Remove original 'MonthOfYear', 'DayOfWeek', and 'Hour' columns to avoid duplication\n",
    "  X_t <- subset(X_t, select = -c(MonthOfYear, DayOfWeek, Hour))\n",
    "  \n",
    "  # Create dummy variables for all factor columns (excluding reference levels)\n",
    "  X_t <- model.matrix(~ . - 1, data = X_t)\n",
    "  \n",
    "  # Find the column indices for numerical columns AFTER creating dummy variables\n",
    "  num_cols <- grep(\"^(Electric cars|Plug-in hybrid cars|humidity_past1h|temp_mean_past1h|wind_speed_past1h|EL_price)\", colnames(X_t))\n",
    "  \n",
    "  # Standardize selected numerical columns\n",
    "  X_t[, num_cols] <- apply(X_t[, num_cols], 2, \n",
    "                           function(x) (x - min(x)) / (max(x) - min(x)))\n",
    "  \n",
    "  # Return the processed dataframe\n",
    "  return(as.data.frame(X_t))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ac3c0-a1be-46b9-afb4-801d08207c1b",
   "metadata": {},
   "source": [
    "### Lag and Align data by \\\\(h\\\\) (horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31834c4-e1f5-4d16-826e-47f01beb3aa9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lag_and_align_data <- function(X_t, R_t, h = 1) {\n",
    "  # Validate inputs\n",
    "  if (!is.numeric(R_t)) {\n",
    "    stop(\"R_t should be a numeric vector.\")\n",
    "  }\n",
    "  if (!is.data.frame(X_t) && !is.matrix(X_t)) {\n",
    "    stop(\"X_t should be a dataframe or a matrix.\")\n",
    "  }\n",
    "  if (!is.numeric(h) || h < 1) {\n",
    "    stop(\"h should be a positive integer.\")\n",
    "  }\n",
    "  \n",
    "  # Convert X_t to a dataframe if it's a matrix\n",
    "  if (is.matrix(X_t)) {\n",
    "    X_t <- as.data.frame(X_t)\n",
    "  }\n",
    "  \n",
    "  # Align R_t with the lagged X_t\n",
    "  # Shift R_t by h positions to align with X_t from the previous timestep\n",
    "  R_t_aligned <- R_t[(h + 1):length(R_t)]\n",
    "  \n",
    "  # Keep X_t up to the second to last row, so it aligns with the shifted R_t\n",
    "  X_t_aligned <- X_t[1:(nrow(X_t) - h), ]\n",
    "  \n",
    "  # Return the aligned datasets\n",
    "  list(X_t = X_t_aligned, R_t = R_t_aligned)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4745ac-d17b-4ee0-86d4-f4fc3659b009",
   "metadata": {},
   "source": [
    "### Plot actual vs estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6448bc0d-8729-487b-8b34-81bb77b11d1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_actual_vs_estimated <- function(R_t, R_hat_t, individual) {\n",
    "  # Validate input\n",
    "  if (!is.numeric(R_t) || !is.numeric(R_hat_t)) {\n",
    "    stop(\"R_t and R_hat_t should be numeric vectors.\")\n",
    "  }\n",
    "  if (!is.data.frame(individual)) {\n",
    "    stop(\"individual should be a dataframe.\")\n",
    "  }\n",
    "    \n",
    "  # Create the plot\n",
    "  plot(R_t, type = 'l', col = 'blue', xlab = \"Time\", ylab = \"Value\", \n",
    "       main = \"Actual vs. Estimated Time Series\\nelvarme: %s, zip_code: %s\")\n",
    "  lines(R_hat_t, type = 'l', col = 'red')\n",
    "  legend(\"topleft\", legend = c(\"Actual\", \"Estimated\"), col = c(\"blue\", \"red\"), lty = 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a5cfa-9f68-4746-b77a-a435f99af099",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a4b451-3023-4def-b0dc-5c6a3f3d6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Setting workign directory and loadign data #####\n",
    "base_path <- \"C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Forecasting-energy-consumption/Data Cleaning\"\n",
    "setwd(base_path)\n",
    "data <- fread(paste0(base_path,\"/Output_file.csv\"))\n",
    "MSTL <- fread(paste0(base_path,\"/MSTL_decomp_results.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ff4d5-37c2-40f5-abec-22044c030226",
   "metadata": {},
   "source": [
    "# Loop for rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "948fc14c-ee4e-4e3f-8c69-2fcccf842450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_size    <- 17544 # 2-year training set\n",
    "# train_size  <- 8784  # 1-year training set\n",
    "num_timesteps <- 720\n",
    "h             <- 1\n",
    "total_size    <- nrow(data) - h\n",
    "set.seed(42)\n",
    "\n",
    "tune_grid <- expand.grid(\n",
    "  eta = c(0.01, 0.05, 0.1),\n",
    "  max_depth = c(3, 6, 9),\n",
    "  subsample = c(0.6, 0.8, 1.0),\n",
    "  colsample_bytree = c(0.6, 0.8, 1.0)\n",
    ")\n",
    "\n",
    "# Set up directory paths\n",
    "#path_R <- \"C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Forecasting-energy-consumption/Data/Results/Boosting/Rounds/R_hat\"\n",
    "#path_M <- \"C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Forecasting-energy-consumption/Data/Results/Boosting/Rounds/Metrics\"\n",
    "path_R <- \"C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Forecasting-energy-consumption/Data/Results/Boosting/Round no window/R_hat\"\n",
    "path_M <- \"C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Forecasting-energy-consumption/Data/Results/Boosting/Round no window/Metrics\"\n",
    "\n",
    "# Number of cores for parallel processing\n",
    "no_cores <- detectCores() - 1\n",
    "cl <- makeCluster(no_cores)\n",
    "registerDoParallel(cl)\n",
    "\n",
    "train_size    <- 17544 #2 year training set\n",
    "#train_size    <- 8784  #1 year training set\n",
    "h             <- 1\n",
    "total_size    <- nrow(data)-h\n",
    "num_timesteps <- total_size - train_size\n",
    "\n",
    "\n",
    "nrounds       <- 100\n",
    "set.seed(42) \n",
    "\n",
    "individual <- data\n",
    "X_t <- prepare_X_t(as.data.frame(individual))\n",
    "R_t <- as.matrix(MSTL$Remainder, nrow = nrow(MSTL), ncol = 1)\n",
    "\n",
    "lag_and_align <- lag_and_align_data(X_t, R_t)\n",
    "X_t <- as.matrix(lag_and_align$X_t)\n",
    "R_t <- as.numeric(lag_and_align$R_t)\n",
    "\n",
    "# Simple train-validation split for hyperparameter tuning\n",
    "train_index <- 1:train_size\n",
    "val_index <- (train_size + 1):(train_size + num_timesteps)\n",
    "\n",
    "dtrain <- xgb.DMatrix(data = X_t[train_index, ], label = R_t[train_index])\n",
    "dval <- xgb.DMatrix(data = X_t[val_index, ], label = R_t[val_index])\n",
    "\n",
    "watchlist <- list(train = dtrain, eval = dval)\n",
    "\n",
    "tune_grid <- expand.grid(\n",
    "  eta = c(0.01, 0.05, 0.1),\n",
    "  max_depth = c(3, 6, 9),\n",
    "  subsample = c(0.6, 0.8, 1.0),\n",
    "  colsample_bytree = c(0.6, 0.8, 1.0)\n",
    ")\n",
    "\n",
    "# Initialize progress bar\n",
    "num_models <- length(seq(50, 5000, by = 50))\n",
    "pb <- progress_bar$new(\n",
    "  total = num_models,\n",
    "  format = \"Tuning model [:bar] :percent in :elapsed\"\n",
    ")\n",
    "\n",
    "# Outer loop to iterate through nrounds from 50 to 10,000 in increments of 50\n",
    "for (nrounds in seq(50, 5000, by = 50)) {\n",
    "  pb$tick()\n",
    "  \n",
    "  best_rmse <- Inf\n",
    "  best_params <- NULL\n",
    "  \n",
    "  # Grid search for hyperparameter tuning\n",
    "  for (i in 1:nrow(tune_grid)) {\n",
    "    params <- list(\n",
    "      objective = \"reg:squarederror\",\n",
    "      eta = tune_grid$eta[i],\n",
    "      max_depth = tune_grid$max_depth[i],\n",
    "      subsample = tune_grid$subsample[i],\n",
    "      colsample_bytree = tune_grid$colsample_bytree[i]\n",
    "    )\n",
    "    \n",
    "    xgb_model <- xgb.train(\n",
    "      params = params,\n",
    "      data = dtrain,\n",
    "      nrounds = nrounds,\n",
    "      watchlist = watchlist,\n",
    "      early_stopping_rounds = 5,\n",
    "      verbose = 0\n",
    "    )\n",
    "    \n",
    "    if (xgb_model$best_score < best_rmse) {\n",
    "      best_rmse <- xgb_model$best_score\n",
    "      best_params <- params\n",
    "    }\n",
    "  }\n",
    "\n",
    "  cl <- makeCluster(no_cores)\n",
    "  registerDoParallel(cl)\n",
    "    \n",
    "  # Parallel processing for fitting and generating results\n",
    "  results <- foreach(j = seq(1, nrow(X_t) - train_size, by = num_timesteps), .combine = 'c', .packages = 'xgboost') %dopar% {\n",
    "    start_index <- j\n",
    "    end_index <- j + train_size - 1\n",
    "    \n",
    "    train_X_t <- X_t[start_index:end_index, ]\n",
    "    train_R_t <- R_t[start_index:end_index]\n",
    "    \n",
    "    dtrain <- xgb.DMatrix(data = train_X_t, label = train_R_t)\n",
    "    \n",
    "    xgb_model <- xgb.train(params = best_params, data = dtrain, nrounds = nrounds)\n",
    "    \n",
    "    test_start_index <- end_index + 1\n",
    "    test_end_index <- min(end_index + num_timesteps, total_size)\n",
    "    test_X_t <- X_t[test_start_index:test_end_index, , drop = FALSE]\n",
    "    dtest <- xgb.DMatrix(data = test_X_t)\n",
    "    \n",
    "    test_predictions <- predict(xgb_model, newdata = dtest)\n",
    "    \n",
    "    num_predictions_to_return <- min(num_timesteps, total_size - test_start_index + 1)\n",
    "    return(test_predictions[1:num_predictions_to_return])\n",
    "  }\n",
    "  stopCluster(cl)\n",
    "  \n",
    "  R_hat_t <- unlist(results)\n",
    "  individual_metrics <- calculate_metrics(tail(R_t, n = length(R_t) - train_size), R_hat_t, data)\n",
    "  \n",
    "  # Save metrics and R_hat_t to CSV files\n",
    "  file_name_R_hat_t <- file.path(path_R, paste0(\"h=\", h, \"_steps_ahead=\", num_timesteps, \"_nrounds=\", nrounds, \"_train_size=\", train_size, \"_XGB_R_hat_t.csv\"))\n",
    "  file_name_metrics <- file.path(path_M, paste0(\"h=\", h, \"_steps_ahead=\", num_timesteps, \"_nrounds=\", nrounds, \"_train_size=\", train_size, \"_XGB_Metrics.csv\"))\n",
    "  \n",
    "  write.csv(R_hat_t, file = file_name_R_hat_t, row.names = FALSE)\n",
    "  write.csv(individual_metrics, file = file_name_metrics, row.names = FALSE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7b90a-400f-4389-b6a4-8964bbfed25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32763a3c-740d-446a-a28c-d8dd0a2a49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7632a904-9ed3-49a2-a961-482016892c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9197c08a-e244-45d4-b950-f43682b51157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_size    <- 17544 # 2-year training set\n",
    "# train_size  <- 8784  # 1-year training set\n",
    "num_timesteps <- 720\n",
    "h             <- 1\n",
    "total_size    <- nrow(data) - h\n",
    "set.seed(42)\n",
    "\n",
    "tune_grid <- expand.grid(\n",
    "  eta = c(0.01, 0.05, 0.1),\n",
    "  max_depth = c(3, 6, 9),\n",
    "  subsample = c(0.6, 0.8, 1.0),\n",
    "  colsample_bytree = c(0.6, 0.8, 1.0)\n",
    ")\n",
    "\n",
    "# Set up directory paths\n",
    "#path_R <- \"C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Forecasting-energy-consumption/Data/Results/Boosting/Rounds/R_hat\"\n",
    "#path_M <- \"C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Forecasting-energy-consumption/Data/Results/Boosting/Rounds/Metrics\"\n",
    "path_R <- \"C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Forecasting-energy-consumption/Data/Results/Boosting/Round no window/R_hat\"\n",
    "path_M <- \"C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Forecasting-energy-consumption/Data/Results/Boosting/Round no window/Metrics\"\n",
    "\n",
    "# Number of cores for parallel processing\n",
    "no_cores <- detectCores() - 1\n",
    "cl <- makeCluster(no_cores)\n",
    "registerDoParallel(cl)\n",
    "\n",
    "train_size    <- 17544 #2 year training set\n",
    "#train_size    <- 8784  #1 year training set\n",
    "h             <- 1\n",
    "total_size    <- nrow(data)-h\n",
    "num_timesteps <- total_size - train_size\n",
    "\n",
    "\n",
    "nrounds       <- 100\n",
    "set.seed(42) \n",
    "\n",
    "individual <- data\n",
    "X_t <- prepare_X_t(as.data.frame(individual))\n",
    "R_t <- as.matrix(MSTL$Remainder, nrow = nrow(MSTL), ncol = 1)\n",
    "\n",
    "lag_and_align <- lag_and_align_data(X_t, R_t)\n",
    "X_t <- as.matrix(lag_and_align$X_t)\n",
    "R_t <- as.numeric(lag_and_align$R_t)\n",
    "\n",
    "# Simple train-validation split for hyperparameter tuning\n",
    "train_index <- 1:train_size\n",
    "val_index <- (train_size + 1):(train_size + num_timesteps)\n",
    "\n",
    "dtrain <- xgb.DMatrix(data = X_t[train_index, ], label = R_t[train_index])\n",
    "dval <- xgb.DMatrix(data = X_t[val_index, ], label = R_t[val_index])\n",
    "\n",
    "watchlist <- list(train = dtrain, eval = dval)\n",
    "\n",
    "tune_grid <- expand.grid(\n",
    "  eta = c(0.01, 0.05, 0.1),\n",
    "  max_depth = c(3, 6, 9),\n",
    "  subsample = c(0.6, 0.8, 1.0),\n",
    "  colsample_bytree = c(0.6, 0.8, 1.0)\n",
    ")\n",
    "\n",
    "# Initialize progress bar\n",
    "num_models <- length(seq(50, 5000, by = 50))\n",
    "pb <- progress_bar$new(\n",
    "  total = num_models,\n",
    "  format = \"Tuning model [:bar] :percent in :elapsed\"\n",
    ")\n",
    "\n",
    "# Outer loop to iterate through nrounds from 50 to 10,000 in increments of 50\n",
    "for (nrounds in seq(50, 5000, by = 50)) {\n",
    "  pb$tick()\n",
    "  \n",
    "  best_rmse <- Inf\n",
    "  best_params <- NULL\n",
    "  \n",
    "  # Grid search for hyperparameter tuning\n",
    "  for (i in 1:nrow(tune_grid)) {\n",
    "    params <- list(\n",
    "      objective = \"reg:squarederror\",\n",
    "      eta = tune_grid$eta[i],\n",
    "      max_depth = tune_grid$max_depth[i],\n",
    "      subsample = tune_grid$subsample[i],\n",
    "      colsample_bytree = tune_grid$colsample_bytree[i]\n",
    "    )\n",
    "    \n",
    "    xgb_model <- xgb.train(\n",
    "      params = params,\n",
    "      data = dtrain,\n",
    "      nrounds = nrounds,\n",
    "      watchlist = watchlist,\n",
    "      early_stopping_rounds = 5,\n",
    "      verbose = 0\n",
    "    )\n",
    "    \n",
    "    if (xgb_model$best_score < best_rmse) {\n",
    "      best_rmse <- xgb_model$best_score\n",
    "      best_params <- params\n",
    "    }\n",
    "  }\n",
    "\n",
    "# Number of cores for parallel processing within xgboost\n",
    "no_cores <- detectCores() - 1\n",
    "\n",
    "# Prepare training data\n",
    "dtrain <- xgb.DMatrix(data = X_t[1:train_size, ], label = R_t[1:train_size])\n",
    "\n",
    "# Configure best parameters and include parallel processing\n",
    "best_params$nthread <- no_cores  # Use all available cores\n",
    "\n",
    "# Train the xgboost model\n",
    "xgb_model <- xgb.train(\n",
    "  params = best_params,\n",
    "  data = dtrain,\n",
    "  nrounds = nrounds,\n",
    "  watchlist = watchlist,\n",
    "  early_stopping_rounds = 5,\n",
    "  verbose = 0\n",
    ")\n",
    "\n",
    "# Prepare test data and make predictions\n",
    "test_start_index <- train_size + 1\n",
    "test_end_index <- min(train_size + num_timesteps, total_size)\n",
    "test_X_t <- X_t[test_start_index:test_end_index, , drop = FALSE]\n",
    "dtest <- xgb.DMatrix(data = test_X_t)\n",
    "\n",
    "# Predict using the trained model\n",
    "test_predictions <- predict(xgb_model, newdata = dtest)\n",
    "\n",
    "# Return predictions for the length needed\n",
    "num_predictions_to_return <- min(num_timesteps, total_size - train_size)\n",
    "R_hat_t <- test_predictions[1:num_predictions_to_return]\n",
    "\n",
    "# Calculate individual metrics\n",
    "individual_metrics <- calculate_metrics(tail(R_t, n = length(R_t) - train_size), R_hat_t, data)\n",
    "\n",
    "# Save metrics and R_hat_t to CSV files\n",
    "file_name_R_hat_t <- file.path(path_R, paste0(\"h=\", h, \"_steps_ahead=\", num_timesteps, \"_nrounds=\", nrounds, \"_train_size=\", train_size, \"_XGB_R_hat_t.csv\"))\n",
    "file_name_metrics <- file.path(path_M, paste0(\"h=\", h, \"_steps_ahead=\", num_timesteps, \"_nrounds=\", nrounds, \"_train_size=\", train_size, \"_XGB_Metrics.csv\"))\n",
    "\n",
    "write.csv(R_hat_t, file = file_name_R_hat_t, row.names = FALSE)\n",
    "write.csv(individual_metrics, file = file_name_metrics, row.names = FALSE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c0609-13e0-4972-9bd3-5a19788fb440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
