{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047952dc-e42e-45d3-971a-a5d47c19b201",
   "metadata": {},
   "source": [
    "# 5.5 Random Forrest: tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91f3e8-2082-4f5b-95c1-ba39964fb3e8",
   "metadata": {},
   "source": [
    "Parameter tuning is a very important part of using decision tree regression and RF models, in forecasting. Both the decision tree and RF specific parameters, as mentioned in chapter 3.5, can have significant impact on the model’s accuracy, in this case it is parameters like the depth of the tree, the maximum number of features considered for splitting a node, the minimum number of data points placed in a node before the node is split, and the minimum number of data points allowed in a leaf node. These parameters help control the tradeoff between bias and variance and allows the model to capture non-linear relationships more effectively.\n",
    "\n",
    "For the framework used in this paper, the parameter tuning is done to maintain the balance between model accuracy and computational time, has been tuned on a selected customer group and applied these optimized parameters across all customer groups. This method reduces the computational time significantly while increasing model accuracy, however with more computational resources this tuning should be done for all customer groups individually. For this thesis it has been chosen not to tune for the parameter of the number of trees used in the RF. This parameter has a very large impact on the computational time, but also model accuracy. therefore, this parameter will be tuned separately in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce256f27-0f3c-48b8-bc6d-38d2e67f44b0",
   "metadata": {},
   "source": [
    "# SPACE FOR PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99754ebe-5776-4d87-b1ab-f330d69c561c",
   "metadata": {},
   "source": [
    "# SPACE FOR PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdfcd14-c0c1-4456-b72c-73fd71d985af",
   "metadata": {},
   "source": [
    "# SPACE FOR PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc683a-6a68-458c-b591-aa7553802bef",
   "metadata": {},
   "source": [
    "The figure above shows the performance metrics (MAE and RMSE) for nine RF models with different number of trees.\n",
    "\n",
    "When looking at the RMSE values, starting from 50 trees, there is a decrease in RMSE, reaching its lowest point at 200 trees before starting to increase again. Suggesting that 200 trees provide the optimal balance between bias and variance.\n",
    "\n",
    "In this case, as the number of trees increases from 50 to 200, the model’s variance is likely reduced without introducing a significant bias, thereby improving the model’s accuracy as indicated by the reduction in RMSE. Beyond 200 trees, the RMSE begins to increase again as mentioned showing that, 200 trees are the optimal choice for the parameter reflected by the RMSE's apparent minimum at this number, as the general idea behind RF is that lots of high variance low biased trees combines to generate a low bias low variance forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
