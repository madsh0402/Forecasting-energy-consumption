{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e04f667-5984-4dad-a34d-3d0771b647ab",
   "metadata": {},
   "source": [
    "## 5.3 Preparing customer groups for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e907a-ee72-4928-9b95-dc49e65c8ac5",
   "metadata": {},
   "source": [
    "Data preprocessing is one of the bedrocks for forecasting model’s accuracy. It is crucial for transforming raw data into another format that algorithms can analyze with precision. The numerical features are scaled to neutralize scale disparities and encode categorical variables numerically to ensure the model interprets every data point correctly. These steps are setting the stage for the analysis that follows. After explaining this process, the following chapter will delve into the effect the forecasting horizon has on the model’s prediction accuracy. By doing so, analyze how the model could be used to predict energy consumption across different time spans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f523ca-eb43-422e-8a9f-f01d746a6712",
   "metadata": {},
   "source": [
    "### 5.3.1 Min-max scaling X and creating dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f1129-507a-417f-80f4-17a04231c7ac",
   "metadata": {},
   "source": [
    "\n",
    "The scaling of features and the transformation of categorical variables into a format is done to help the interpreted by algorithms. This section dives into the role of Min-Max scaling in normalizing data, alongside the creation of dummy variables for categorical data, explaining how these processes improve model performance.\n",
    "Scaling features is essential in forecasting models that are sensitive to the magnitude of variables. Disparate scales across features can lead to skewed weight which affects the model’s assignments, whereas models disproportionately favor features with larger magnitudes. This affects the model's ability to generalize from the data for the worse.  Equally crucial is the treatment of categorical data through the creation of dummy variables. Many machine learning algorithms are designed to operate on numerical input and thus cannot directly handle categorical data. Dummy variables serve as a bridge, transforming qualitative data into a binary numerical format. As such enabling the inclusion of categorical predictors in the model. This allows for a broader analysis that incorporates both numerical and categorical influences on the forecasted outcome. As the dataset of this assignment has categorical variables this is an essential part of the model process.\n",
    "Moreover, the use of dummy variables preserves the nominal nature of the data without imposing an artificial relationship, which could mislead the model. This method ensures that the categorical data is accurately represented, allowing the model to discern the distinct influence of each category on the predictive outcome.\n",
    "The conversion of categorical variables into dummy or indicator variables process involves the following steps:\n",
    "\n",
    "1. **Identification of Categorical Variables:** Recognize all categorical variables        within the dataset that cannot be directly interpreted by numerical models.\n",
    "2. **One-Hot Encoding**: Implement one-hot encoding, a technique where each      category value is converted into a new binary column. For each record, the presence of a category is marked by 1, and 0 signifies its absence.\n",
    "3. **Avoiding Dummy Variable Trap**: Ensure that for each categorical feature with  N categories, N-1 dummy variables are created to prevent multicollinearity. \n",
    "    \n",
    "Now that the process of making dummy variables has been explained another problem arises in the dataset. The problem is that there are variables that have different scales, therefore Min-Max scaling is used. Min-Max is a normalization technique that adjusts the features of the data to a common scale, ranging from 0 to 1. This method transforms each feature by subtracting the minimum value of the feature and then divide by the range of the feature values. The formula for Min-Max scaling is given by:\n",
    "\n",
    "$$ x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}  $$\n",
    "\n",
    "In this equation, \\\\( x' \\\\) denotes the scaled value, \\\\( x \\\\) represents the original value, \\\\( \\min(x) \\\\) is the minimum value across the feature, and \\\\( \\max(x) \\\\) is the maximum value. This transformation ensures that no single feature disproportionately influences the model due to its scale. This makes the model less sensitive to the scale of features and thus more capable of identifying the true underlying patterns in the data. By aligning the scales of the features, Min-Max scaling helps in stabilizing the convergence of algorithms and facilitating a more balanced weight assignment across features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6eec78-e3f7-454b-b9ed-543e6b6b5402",
   "metadata": {},
   "source": [
    "### 5.3.2 Detrending and deseasonalizing target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edbcdb3-c94b-4fa1-adad-72410724b052",
   "metadata": {},
   "source": [
    "In earlier chapters (3.1), the theoretical foundations of time series analysis were investigated, focusing on the decomposition of time series data into its fundamental components: trend (\\\\( T \\\\)), seasonality (\\\\( S^k \\\\)), and remainders (\\\\( R \\\\)). This section builds upon that foundation, specifically addressing the practical implications and the method used for detrending and deseasonalizing the target variable within the forecasting model framework. The reason behind these preprocessing steps is to make the predictability of the model better by isolating and removing systematic patterns that can obscure the underlying signal in the time series data.\n",
    "\n",
    "The presence of trends and seasonal patterns in time series data introduces systematic variations that can significantly influence the performance of forecasting models. As mentioned in chapter 3.1 trends represent long-term movements in data, either upwards or downwards, while seasonality indicates regular, predictable patterns that occurs at specific intervals within the data. By removing these components, the model's ability to capture the true underlying behaviors of the time series improves its predictability and accuracy. \n",
    "\n",
    "Detrending and deseasonalizing are accomplished through various mathematical and algorithmic approaches, with the MSTL decomposition standing out as a foundational method. MSTL decomposition separates the time series into trend, seasonal, and remaining components through procedure. For a more detailed look at the theoretical explanation of MSTL.\n",
    "\n",
    "To ensure the correct application of the detrending and deseasonalizing processes, the component separation is verified after applying MSTL decomposition, and the extracted trend and seasonal components is assessed. The process of detrending and deseasonalizing directly contributes to making the target variable more stationary. By removing trends and seasonal fluctuations, these processes reduce systematic changes over time, thereby stabilizing the mean and reducing variations within the series. This transformation allows predictive models to perform more effectively, as they can focus on modeling the remainders, which ideally is not random noise, improving the model’s ability to forecast future values with greater accuracy. It may even make sure that the horizon of prediction in time does not have that large of an effect on the accuracy and thereby hopefully make the impact on the prediction accuracy low such the model can be used at different forecasting horizons. \n",
    "\n",
    "Later on, the application of OLS and RF is used on the remainders to investigate if these methods can accurately predict the remainders. In the completed evaluation, the selection of the model that performs the best will be chosen and then combined with models from MSTL decomposition. The aim is to achieve the optimal outcome and thereby find the most accurate prediction model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f8ebe2-e80f-4911-a0ef-f6d1b27fe61f",
   "metadata": {},
   "source": [
    "### 5.3.3. Lagging X to forecast into future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacee8b9-5d83-423a-8225-7eefd4b945e1",
   "metadata": {},
   "source": [
    "In prediction, the selection of the right forecasting horizon (\\\\( h \\\\)) is a trade-off between the specific purpose of the forecast and the effect the accuracy the horizon has on the forecasts. This initial choice of different horizons can substantially affect forecast accuracy and model performance. Understanding the relationship between the forecasting horizon and its impact on predictiveness can help choose the best model calibration. This section aims to analyze the significance of forecasting horizons within the context of using OLS for predictive analysis and how strategic lagging of variables (\\\\( X \\\\)) can improve forecasting into the future.\n",
    "\n",
    "For this study, the selected forecasting horizons (\\\\( h \\\\)) are 1, 24, 168, 720, 2160, and 4320 hours. These intervals are chosen to encompass a broad spectrum of forecasting needs, ranging from short-term (1 hour) to long-term forecasts (4320 hours), allowing for a broad analysis across different time frames. The rationale behind selecting these specific intervals lies in their relevance to operational and strategic decision-making processes within Andel Energi. It should be noted that the assumption is that the effect of the forecasting horizon has the same effect on the RF model. The reason behind this assumption is based on how much the heavy computational cost is and the time it would take as a result.\n",
    "\n",
    "The study employs several accuracy metrics to evaluate the forecasting performance, including Mean Absolute Error (*MAE*), Root Mean Squared Error (*RMSE*), and the coefficient of determination (\\\\( R^2 \\\\)). Each metric offers a unique perspective which helps to assess the accuracy and reliability of the forecasting model, to summarize: \n",
    "\n",
    "* *MAE* measures the average magnitude of the errors in a set of predictions, without considering their direction. It's a straightforward metric that provides insight into the average error magnitude.\n",
    "\n",
    "$$ MAE = \\frac{1}{N}\\sum_{t=n_{train+1}}^{T} |R_t - \\hat{R_t}| $$  \n",
    "\n",
    "* *RMSE* provides a measure of the square root of the average squared differences between predicted and actual values, emphasizing larger errors more significantly than MAE, thereby penalizing substantial deviations more heavily.\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{1}{N}\\sum_{t=n_{train+1}}^{T} (R_t - \\hat{R_t})^2} $$\n",
    "\n",
    "Together, these metrics give a framework for evaluating the performance of the forecasting model across different horizons, facilitating an informed selection of the most appropriate model configurations for accurate future predictions. For a more detailed description please see chapter 3.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60991743-69c4-40e2-b887-659c9d03e18f",
   "metadata": {},
   "source": [
    "### 5.3.4 The impact of forecasting horizon (\\\\( h \\\\))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca16f4-70e6-45f1-8acc-b262b8b9eb44",
   "metadata": {},
   "source": [
    "This section will look into how varying the length of the forecasting horizon affects the performance of OLS models. The forecasting horizon, represented as \\\\( h \\\\), refers to the future time period over which predictions are made, ranging from very short (1 hour) to long-term forecasts (4320 hours). By examining OLS models across these diverse horizons, the aim is to find a trend in prediction accuracy.\n",
    "\n",
    "To better understand which impact the forecasting horizon (\\\\( h \\\\)) has on model performance, there was trained and tested for various OLS models across a spectrum of horizons, specifically at 1, 24, 168, 720, 2160, and 4320 hours. To interpret the differences, boxplots were used as a visualization tool, because of its properties in showing the distribution and different percentile intervals for our accuracy metrics. After running the OLS models at different horizons, the following visualization of the distribution for the accuracy metrics were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb1cd21-5d73-4b22-964b-c441b3e93185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
