{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a751aa9-f5ab-4d4b-b732-8a09f2de09b7",
   "metadata": {},
   "source": [
    "## 6.5 Further research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c210c-5552-461f-afad-de3510fb73c3",
   "metadata": {},
   "source": [
    "Although the practical implications and challenges discussed above represent some of the limitations of this study, they can also guide toward possible directions for future research that could improve the model showcased in the paper. Several common challenges can arise in modeling scenarios: overfitting in OLS models may skew results, and variable selection in RF could lead to inaccuracies. Thus, one possibility for future research might be to try some regularization techniques such as Lasso or Ridge regression. Regularization can help combat overfitting by punishing the size of the coefficients and consequently penalizing the modelâ€™s complexity. It is also possible to research the effect of regularization techniques on model efficiency, which variables were kept, and which were erased, as well as whether the prediction accuracy has changed.\n",
    "\n",
    "Given that OLS models in the thesis underperform compared to the benchmark and RF could be because of their limited capacity to manage numerous exogenous variables compared to RF. Therefore, it might be beneficial to explore boosting algorithms, which are designed to improve model accuracy by focusing on correcting the mistakes of previous models. Boosting, which is an effective algorithm, has the potential to outperform by decreasing propensity and variance, while also making sequential improvements to the predictions of several-weak learners, essentially making boosting an improvement of RF. Moreover, the efficiency of models built using the boosted gradient as opposed to RF might prove interesting and lead to other methods.\n",
    "\n",
    "To address the challenge of having too many variables, PCA could be used to reduce dimensionality while retaining the most significant variables. This technique could help in understanding which variables explain the most variance and are thus the most important for the models. Further research could explore how the reduced variable set affects model performance, particularly in OLS and RF, and whether it helps in reducing overfitting and might also help concerning computation cost and time spent training and validating models.\n",
    "\n",
    "By including lagged \\\\(Y_t\\\\) to the independent variables it could potentially improve the model's ability to capture autocorrelation in the time series, which MSTL might have missed. Lagged values of the response variable can provide additional information about the previous state, which could capture some of the past performance which could indicate future outcomes. Similarly, the inclusion of lagged \\\\(R_t\\\\) could further improve the model. The remainders from the MSTL model as stated before represent the part of the data that does not fit into the seasonal and trend patterns and may contain information about undiscovered recurrent variations. By adding lagged \\\\(R_t\\\\), it might capture and exploit these dynamics, which could in turn produce a better prediction model. The implementation of lagged \\\\(Y_t\\\\) and \\\\(R_t\\\\) in the OLS and RF models could improve the prediction. However, it also requires some consideration of which lags to include to maximize the predictive capabilities of the models without introducing multicollinearity or overfitting. This, however, could be analyzed through model validation methods such as cross-validation and information criteria like AIC and BIC to determine the optimal lag structure, which however would come with a substantial computational cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
