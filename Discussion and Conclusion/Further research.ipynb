{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a751aa9-f5ab-4d4b-b732-8a09f2de09b7",
   "metadata": {},
   "source": [
    "## 6.5 Further research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c210c-5552-461f-afad-de3510fb73c3",
   "metadata": {},
   "source": [
    "Although the practical implications and challenges discussed above represent some of the limitations of this study, they can also guide toward possible directions for future research that could improve the model showcased in the paper. Several common challenges can arise in modeling scenarios: overfitting in OLS models may skew results, and variable selection in RF could lead to inaccuracies. Thus, one possibility for future research might be to try some regularization techniques such as Lasso or Ridge regression. Regularization can help combat overfitting by punishing the size of the coefficients and consequently penalizing the model‚Äôs complexity. It is also possible to research the effect of regularization techniques on model efficiency, which variables were kept, and which were erased, as well as whether the prediction accuracy has changed.\n",
    "\n",
    "Given that OLS models in the thesis underperform compared to the benchmark and RF could be because of their limited capacity to manage numerous exogenous variables compared to RF. Therefore, it might be beneficial to explore boosting algorithms, which are designed to improve model accuracy by focusing on correcting the mistakes of previous models. Boosting, which is an effective algorithm, has the potential to outperform by decreasing propensity and variance, while also making sequential improvements to the predictions of several-weak learners, essentially making boosting an improvement of RF. Moreover, the efficiency of models built using the boosted gradient as opposed to RF might prove interesting and lead to other methods.\n",
    "\n",
    "To address the challenge of having too many variables, PCA could be used to reduce dimensionality while retaining the most significant variables. This technique could help in understanding which variables explain the most variance and are thus the most important for the models. Further research could explore how the reduced variable set affects model performance, particularly in OLS and RF, and whether it helps in reducing overfitting and might also help concerning computation cost and time spent training and validating models.\n",
    "\n",
    "By including lagged \\\\(\\\\)ùëåùë° to the independent variables it could potentially improve the model's ability to capture autocorrelation in the time series, which MSTL might have missed. Lagged values of the response variable can provide additional information about the previous state, which could capture some of the past performance which could indicate future outcomes. Similarly, the inclusion of lagged ùëÖùë° could further improve the model. The remainders from the MSTL model as stated before represent the part of the data that does not fit into the seasonal and trend patterns and may contain information about undiscovered recurrent variations. By adding lagged ùëÖùë°, it might capture and exploit these dynamics, which could in turn produce a better prediction model. The implementation of lagged ùëåùë° and ùëÖùë° in the OLS and RF models could improve the prediction. However, it also requires some consideration of which lags to include to maximize the predictive capabilities of the models without introducing multicollinearity or overfitting. This, however, could be analyzed through model validation methods such as cross-validation and information criteria like AIC and BIC to determine the optimal lag structure, which however would come with a substantial computational cost.\n",
    "\n",
    "Finally, a complete combination of model types, such as linking an econometric model with a machine learning model, could be done to use the advantages of different models and, as a result, get a better forecasting model. Another aspect that could be interesting to look at is fine-tuning parameters for each individual customer group, such as in a RF algorithm. This could improve the model's performance. This approach could be valuable because the different geographical destinations and different socioeconomic parameters might result in diverse customer segments where a one-size-fits-all model, which is essential to obtain when tuning for only one customer group, might not be able to capture the unique characteristics and behaviors of different groups. Therefore, it could be interesting to fit the parameters to each customer group and see how big of an impact it might have. This Framework involves adjusting the parameters of the RF model for each customer group separately, by going through each customer group before the rolling window approach. However, this method is computationally expensive and time-consuming, especially when tuning parameters for as many as 707 customer groups. One of the large constraints in this thesis has been the time constraint and finding enough computational power. However, if this fine-tuning is done and it shows promising results it could be worthwhile for Andel Energi to invest in more heavy computational power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
